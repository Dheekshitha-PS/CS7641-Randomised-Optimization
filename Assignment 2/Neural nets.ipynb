{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/hiive/mlrose/blob/master/problem_examples.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discrete optimization problems:\n",
    "Dataset: it's just a discrete problem, a function we’re trying to maximize or minimize.  There isn’t a dataset. They are usually represented as bit strings.\n",
    "Make sure we have one problem to highlight SA, MIMIC and GA (no need for one for RHC): \n",
    "algos:\n",
    "randomized hill climbing (r.h.c)\n",
    "simulated annealing (s.a.)\n",
    "a genetic algorithm (g.a.)\n",
    "MIMIC\n",
    "Talk about the problem, why you think it's interesting, how many local optima does it have, how many global optima does it have. \n",
    "Talk about the structure of the problem and how algorithms may exploit it, or how the structure is more simple and something like s.a. Will do better. \n",
    "Do hyperparameter tuning for each algorithm for each problem, for a given problem size. The problem size should be picked so it highlights the specific algorithm. In addition to focusing on one problem size, you should explore others. It's through this exploration you find this “ideal” problem size for the strongest-algorithm for that problem type. They want to see this exploration and see reports on the results for other problem sizes. \n",
    "Talk about how you think performance may change if you continue to tune hyperparameters in some way\n",
    "Convergence property - getting to the best answer and getting to a reasonable amount of time. We should also compare in terms of time to converge in addition to fitness value. This is performance vs # iteration. Convergence vs problem size. \n",
    "\n",
    "\n",
    "Neural network:\n",
    "Fitness minimization problem\n",
    "Replace back propagation with s.a., r.h.c, and genetic (no MIMIC). Compare with backprop, why you got the result, and compare in terms of performance metric and convergence time and behavior. \n",
    "We should think about the advantages of back prop, the advantages that these optimization problems have, note that these are for discrete space and back prop is for continuous and smooth space. What if the space is smooth vs not. \n",
    "Talk about why one outperforms the other. \n",
    "Hyperparameter tuning - they don’t expect us to change the structure of the network unless we need to get it to work on mlrose. We may need to tune the randomized optimization (r.o.) methods  to get them to work with the NN. We can tune the NN with the r.o. as well as needed to get them to work with these r.o. problems. Structure can stay the same. \n",
    "Conclusion:\n",
    "We have seen optimization algorithms, we’ve seen which performs better, worse, explain tradeoffs and observations. \n",
    "\n",
    "Results just give you the ability to do analysis - make sure that you analyze them.\n",
    "\n",
    "Resources: \n",
    "mlrose-hive documentation: https://mlrose.readthedocs.io/\n",
    "GARunner: https://github.com/hiive/mlrose/tree/master/mlrose_hiive/runners \n",
    "Example using runners: https://github.com/hiive/mlrose/blob/master/problem_examples.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
