{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fbfkjb)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import re\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#############################\n",
    "### Get Data ##\n",
    "\n",
    "# train= pd.read_csv('train.csv')\n",
    "# test=pd.read_csv('test.csv')\n",
    "# # concat all\n",
    "# df=pd.concat([train,test])\n",
    "\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# def correct_spellings(x, spell=spell):\n",
    "#     \"\"\"correct the missplled words of a given tweet\"\"\"\n",
    "#     x = x.split()\n",
    "#     misspelled = spell.unknown(x)\n",
    "#     result = map(lambda word : spell.correction(word) if word in  misspelled else word, x)\n",
    "#     return \" \".join(result)\n",
    "\n",
    "def tweets_cleaning(x, correct_spelling=False, remove_emojis=True, remove_stop_words=True):\n",
    "    \"\"\"Apply function to a clean a tweet\"\"\"\n",
    "    x = x.lower().strip()\n",
    "    # romove urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    x = url.sub(r'',x)\n",
    "    # remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    x = html.sub(r'',x)\n",
    "    # remove punctuation\n",
    "    operator = str.maketrans('','',string.punctuation) #????\n",
    "    x = x.translate(operator)\n",
    "    if correct_spelling:\n",
    "        x = correct_spellings(x)\n",
    "    if remove_emojis:\n",
    "        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n",
    "    if remove_stop_words:\n",
    "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_train=pd.read_csv(\"TEXT_X_train.csv\")\n",
    "# x_test=pd.read_csv(\"TEXT_X_test.csv\")\n",
    "\n",
    "\n",
    "y_train=pd.read_csv(\"TEXTlabel_train.csv\")\n",
    "y_test =pd.read_csv(\"TEXTlabel_test .csv\") \n",
    "\n",
    "# x_train.shape\n",
    "\n",
    "# # X_test=X_test.drop('Unnamed: 0',axis=1)\n",
    "# y_train=pd.read_csv(\"TEXTlabel_train.csv\")\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test=x_test.drop('Unnamed: 0',axis=1)\n",
    "# x_train=x_train.drop('Unnamed: 0',axis=1)\n",
    "y_test=y_test.drop('Unnamed: 0',axis=1)\n",
    "y_train=y_train.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv=pd.read_csv(\"TEXT_X_V.csv\")\n",
    "y_cv=pd.read_csv(\"TEXTlabel_V.csv\")\n",
    "x_cv=x_cv.drop('Unnamed: 0',axis=1)\n",
    "y_cv=y_cv.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(  hidden_layers,learning_rate):\n",
    "#   # Initialize the constructor\n",
    "#     model =  keras.Sequential()\n",
    "#       # Add an input layer\n",
    "#     activation='sigmoid'\n",
    "#     model.add(layers.Dense(256, activation=activation, input_shape=(512,)))\n",
    "#     for i in range(hidden_layers):\n",
    "#           # Add one hidden layer\n",
    "#         model.add(layers.Dense(8, activation=activation))\n",
    "\n",
    "#       # Add an output layer \n",
    "#     model.add(layers.Dense(1, activation=activation))\n",
    "#     opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#       #compile model\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=\n",
    "#       ['accuracy'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(DTmodel, 'DTgrid_NLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tempfile import TemporaryFile\n",
    "\n",
    "# outfile = TemporaryFile()\n",
    "xtrain_tfidf_ngram=np.load(\"xtrain_tfidf_ngram.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _ = outfile.seek(0) # Only needed here to simulate closing & reopening file\n",
    "\n",
    "# np.load(outfile)\n",
    "xvalid_tfidf_ngram=np.load(\"xvalid_tfidf_ngram.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_X = xtrain_tfidf_ngram  #//shape - (3,6)\n",
    "tfidf_vector_valid= xvalid_tfidf_ngram\n",
    "# None] #//shape - (3,6,1) \n",
    "# tfidf_vector_valid = tfidf_vector_valid[:, :, None] #//shape - (3,6,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML # for some notebook formatting.\n",
    "\n",
    "import mlrose_hiive\n",
    "import numpy as np\n",
    "import logging\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from mlrose_hiive import QueensGenerator, MaxKColorGenerator\n",
    "# from mlrose_hiive import SARunner, GARunner, NNGSRunner\n",
    "\n",
    "# switch off the chatter\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "from mlrose_hiive import NNGSRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population_sizes=[10, 20, 50,100,150,200,300]\n",
    "# mutation_rates=np.arange(0,0.4,0.015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrose_hiive.algorithms.genetic_alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1227, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1227, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_parameters = {\n",
    "    'max_iters': [1000],                          # nn params\n",
    "    'learning_rate': [0.1],                       # nn params\n",
    "    'activation': [mlrose_hiive.relu],            # nn params\n",
    "   'temperature_list':[100000,10000,1000000,10000000,100000000],\n",
    "              'decay_list':[mlrose_hiive.GeomDecay(init_temp=100000000000, decay=0.95, min_temp=1)]\n",
    "}\n",
    "\n",
    "nnr = NNGSRunner(\n",
    "    x_train=xtrain_tfidf_ngram, \n",
    "    \n",
    "    y_train=y_train,\n",
    "    x_test=xvalid_tfidf_ngram,\n",
    "    y_test=y_cv,\n",
    "    experiment_name='TEXT_nn_test_SA_grid',\n",
    "    algorithm=mlrose_hiive.algorithms.genetic_alg,\n",
    "    grid_search_parameters=grid_search_parameters,\n",
    "    iteration_list=[100],\n",
    "    hidden_layer_sizes=[[2]],\n",
    "    bias=True,\n",
    "    early_stopping=True,\n",
    "    clip_max=5,\n",
    "    max_attempts=500,\n",
    "    n_jobs=5,\n",
    "    seed=123456,\n",
    "    output_directory=\"Neural networks\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Runtime of the program is 5313.605382680893\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "\n",
    "run_stats_df, curves_df, cv_results_df, grid_search_cv = nnr.run()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Runtime of the program is 17.585942029953003\n"
     ]
    }
   ],
   "source": [
    "grid_search_parameters = {\n",
    "    'max_iters': [1000],                          # nn params\n",
    "    'learning_rate': [0.1],                       # nn params\n",
    "    'activation': [mlrose_hiive.relu],            # nn params\n",
    "   'temperature_list':[1000000],\n",
    "              'decay_list':[mlrose_hiive.GeomDecay(init_temp=10000000000, decay=0.95, min_temp=1)]\n",
    "}\n",
    "\n",
    "nnr = NNGSRunner(\n",
    "    x_train=xtrain_tfidf_ngram, \n",
    "    \n",
    "    y_train=y_train,\n",
    "    x_test=xvalid_tfidf_ngram,\n",
    "    y_test=y_cv,\n",
    "    experiment_name='TEXT_nn_test_SA_tuned',\n",
    "    algorithm=mlrose_hiive.algorithms.simulated_annealing,\n",
    "    grid_search_parameters=grid_search_parameters,\n",
    "    iteration_list=[100],\n",
    "    hidden_layer_sizes=[[2]],\n",
    "    bias=True,\n",
    "    early_stopping=True,\n",
    "    clip_max=5,\n",
    "    max_attempts=500,\n",
    "    n_jobs=5,\n",
    "    seed=123456,\n",
    "    output_directory=\"Neural networks\"\n",
    ")\n",
    "import time\n",
    "start=time.time()\n",
    "\n",
    "run_stats_df, curves_df, cv_results_df, grid_search_cv = nnr.run()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 44.44444444444444 %\n",
      "recall_score: 5.555555555555555 %\n",
      "0.6430317848410758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv, grid_search_cv.predict(xvalid_tfidf_ngram))*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv, grid_search_cv.predict(xvalid_tfidf_ngram))*100,'%')\n",
    "\n",
    "y_test_pred = grid_search_cv.predict(xvalid_tfidf_ngram)\n",
    "y_test_accuracy = accuracy_score(y_cv, y_test_pred)\n",
    "print(y_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_stats_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-058d7f2a2c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_stats_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_stats_df' is not defined"
     ]
    }
   ],
   "source": [
    "run_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 44.44444444444444 %\n",
      "recall_score: 5.555555555555555 %\n"
     ]
    }
   ],
   "source": [
    "print('precision_score:',metrics.precision_score(y_cv, grid_search_cv.predict(xvalid_tfidf_ngram))*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv, grid_search_cv.predict(xvalid_tfidf_ngram))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6430317848410758\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = grid_search_cv.predict(xvalid_tfidf_ngram)\n",
    "y_test_accuracy = accuracy_score(y_cv, y_test_pred)\n",
    "print(y_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape= [x_train.shape[1]]\n",
    "\n",
    "# NNmodel = create_model(optimizer='adam', activation = 'sigmoid', hidden_layers=1,learning_rate=0.01)\n",
    "\n",
    "# # Create model\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(256, activation='relu', input_shape=tfidf_vector_X.shape[1:]),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "    \n",
    "#     layers.Dense(256, activation='relu'),    \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "    \n",
    "#     layers.Dense(1, activation='sigmoid'),\n",
    "# ])\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=opt,\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['binary_accuracy'],\n",
    "    \n",
    "# )\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "model =  KerasClassifier(build_fn=create_model, verbose = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'learning_rate':[0.1,0.001,0.01,0.05,0.05,0.5],'hidden_layers':[2,4,6,8,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridNN = GridSearchCV(estimator = model, param_grid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = gridNN.fit(\n",
    "     xtrain_tfidf_ngram, y_train,\n",
    "    validation_data=(xvalid_tfidf_ngram,y_cv),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "#     callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.best_score_)\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gridNN.predict(xvalid_tfidf_ngram)\n",
    "\n",
    "# Plotting\n",
    "cm = metrics.confusion_matrix(y_cv, predictions)\n",
    "# plt.figure(figsize=(9,9))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "# plt.ylabel('Actual label');\n",
    "# plt.xlabel('Predicted label');\n",
    "# all_sample_title = 'Accuracy Score: {0}%'.format(round(history_df['accuracy'].iloc[-1]*100,1))\n",
    "# plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, gridNN.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, gridNN.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, gridNN.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, gridNN.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Confusion matrix: \\n', cm)\n",
    "print('TP: ', cm[1,1])\n",
    "print('TN: ', cm[0,0])\n",
    "print('FP: ', cm[0,1])\n",
    "print('FN: ', cm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_test, model))\n",
    "print('Accuracy_Score Test:',metrics.accuracy_score(y_test, gridNN.predict(xvalid_tfidf_ngram))*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('precision_score:',metrics.precision_score(y_cv, gridNN.predict(xvalid_tfidf_ngram))*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv, gridNN.predict(xvalid_tfidf_ngram))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelWithDropBatch(  hidden_layers,learning_rate):\n",
    "  # Initialize the constructor\n",
    "    model =  keras.Sequential()\n",
    "      # Add an input layer\n",
    "    activation='sigmoid'\n",
    "    model.add(layers.Dense(256, activation=activation, input_shape=(512,)))\n",
    "    for i in range(hidden_layers):\n",
    "          # Add one hidden layer\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(8, activation='relu'))\n",
    "\n",
    "      # Add an output layer \n",
    "    model.add(layers.Dense(1, activation=activation))\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "      #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=\n",
    "      ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(  hidden_layers,learning_rate):\n",
    "  # Initialize the constructor\n",
    "    model =  keras.Sequential()\n",
    "      # Add an input layer\n",
    "    activation='sigmoid'\n",
    "    model.add(layers.Dense(256, activation=activation, input_shape=(512,)))\n",
    "    for i in range(hidden_layers):\n",
    "          # Add one hidden layer\n",
    "        model.add(layers.Dense(8, activation=activation))\n",
    "\n",
    "      # Add an output layer \n",
    "    model.add(layers.Dense(1, activation=activation))\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "      #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=\n",
    "      ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model_Batch_drop_relu =  KerasClassifier(build_fn=create_modelWithDropBatch, verbose = False)  \n",
    "\n",
    "param_grid={'learning_rate':[0.1,0.001,0.01,0.05,0.05,0.5],'hidden_layers':[2,4,6,8,10]}\n",
    "\n",
    "gridNN_Batch_drop_relu = GridSearchCV(estimator = model_Batch_drop_relu, param_grid = param_grid)\n",
    "\n",
    "model_Batch_drop_relu.get_params()\n",
    "\n",
    "history_Batch_drop_relu = gridNN_Batch_drop_relu.fit(\n",
    "     xtrain_tfidf_ngram, y_train,\n",
    "    validation_data=(xvalid_tfidf_ngram,y_cv),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "#     callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "history_Batch_drop_relu.best_params_\n",
    "\n",
    "print(history_Batch_drop_relu.best_score_)\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM1 = gridNN_Batch_drop_relu.predict(xvalid_tfidf_ngram)\n",
    "y_pred_SVM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM = gridNN_Batch_drop_relu.predict(xvalid_tfidf_ngram)\n",
    "\n",
    "conf_mat_svm = metrics.confusion_matrix(y_cv['label'], y_pred_SVM)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_mat_svm,annot=True)\n",
    "plt.title(\"Confusion_matrix\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"Actual class\")\n",
    "plt.show()\n",
    "print('Confusion matrix: \\n', conf_mat_svm)\n",
    "print('TP: ', conf_mat_svm[1,1])\n",
    "print('TN: ', conf_mat_svm[0,0])\n",
    "print('FP: ', conf_mat_svm[0,1])\n",
    "print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "\n",
    "# print('Confusion matrix: \\n', cm)\n",
    "print('TP: ', conf_mat_svm[1,1])\n",
    "print('TN: ', conf_mat_svm[0,0])\n",
    "print('FP: ', conf_mat_svm[0,1])\n",
    "print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_test, model))\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv['label'], y_pred_SVM)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv['label'], y_pred_SVM)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv['label'], y_pred_SVM)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, gridNN_Batch_drop_relu.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, gridNN_Batch_drop_relu.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, gridNN_Batch_drop_relu.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, gridNN_Batch_drop_relu.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestNN=create_modelWithDropBatch(hidden_layers= 2, learning_rate= 0.05)\n",
    "history2 = bestNN.fit(\n",
    "     xtrain_tfidf_ngram, y_train,\n",
    "    validation_data=(xvalid_tfidf_ngram,y_cv),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "#     callbacks=[early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history2.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# modelwithDropout = keras.Sequential([\n",
    "#     layers.Dense(256, activation='relu', input_shape=input_shape),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "    \n",
    "#     layers.Dense(256, activation='relu'),    \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "    \n",
    "#     layers.Dense(1, activation='sigmoid'),\n",
    "# ])\n",
    "\n",
    "# modelwithDropout.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['binary_accuracy'],\n",
    "# )\n",
    "\n",
    "bestNNearly=create_modelWithDropBatch(hidden_layers= 2, learning_rate= 0.05)\n",
    "history3 = bestNNearly.fit(\n",
    "     xtrain_tfidf_ngram, y_train,\n",
    "    validation_data=(xvalid_tfidf_ngram,y_cv),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history3.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history3.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNN.predict(xtrain_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_SVM = bestNNearly.predict_classes(xvalid_tfidf_ngram)\n",
    "\n",
    "# conf_mat_svm = metrics.confusion_matrix(y_cv['label'], y_pred_SVM)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(conf_mat_svm,annot=True)\n",
    "# plt.title(\"Confusion_matrix\")\n",
    "# plt.xlabel(\"Predicted Class\")\n",
    "# plt.ylabel(\"Actual class\")\n",
    "# plt.show()\n",
    "# print('Confusion matrix: \\n', conf_mat_svm)\n",
    "# print('TP: ', conf_mat_svm[1,1])\n",
    "# print('TN: ', conf_mat_svm[0,0])\n",
    "# print('FP: ', conf_mat_svm[0,1])\n",
    "# print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_test, model))\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv['label'], y_pred_SVM)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv['label'], y_pred_SVM)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv['label'], y_pred_SVM)*100,'%')\n",
    "\n",
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, bestNNearly.predict_classes(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, bestNNearly.predict_classes(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, bestNNearly.predict_classes(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, bestNNearly.predict_classes(xtrain_tfidf_ngram))*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=pd.read_csv(\"TEXT_X_test.csv\")\n",
    "\n",
    "# x_cv=pd.read_csv(\"TEXT_X_V.csv\")\n",
    "# y_cv=pd.read_csv(\"TEXTlabel_V.csv\")\n",
    "# y_train=pd.read_csv(\"TEXTlabel_train.csv\")\n",
    "y_test =pd.read_csv(\"TEXTlabel_test .csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tfidf_ngram=np.load(\"xtest_tfidf_ngram.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing Accuracy_Score:',metrics.accuracy_score(y_test['label'], bestNNearly.predict_classes(xtest_tfidf_ngram))*100,'%')\n",
    "print('testing Recall:',metrics.recall_score(y_test['label'], bestNNearly.predict_classes(xtest_tfidf_ngram))*100,'%')\n",
    "print('testing precision_score:',metrics.precision_score(y_test['label'], bestNNearly.predict_classes(xtest_tfidf_ngram))*100,'%')\n",
    "print('testing F1 Score:',metrics.f1_score(y_test['label'], bestNNearly.predict_classes(xtest_tfidf_ngram))*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ahfhfkj)\n",
    "# y_train_res=y_train_res_01.map({ 1:'Existing Customer',  0:'Attrited Customer'})\n",
    "\n",
    "parameter_xgb= [0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "   xgb.XGBClassifier(learning_rate=0.1, n_estimators=50,booster='dart',\n",
    "#                              subsample= 1.0,\n",
    "              max_depth= 3, gamma= 0.5, \n",
    "#                      colsample_bytree= 1.0,\n",
    "                            \n",
    "                            nthread=1, use_label_encoder=False, eval_metric=[\"error\", \"logloss\"]), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_xgb,  scoring=\"precision\",param_name=\"subsample\",cv=5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range=parameter_xgb\n",
    "plt.title(\"Validation Curve with XGB\")\n",
    "plt.xlabel(\"subsample\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "# lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "sizes, training_scores, testing_scores = learning_curve(SVC(C=10,gamma=0.1,kernel='rbf'),xtrain_tfidf_ngram, y_train['label'])\n",
    "\n",
    "# Mean and Standard Deviation of training scores \n",
    "mean_training = np.mean(training_scores, axis=1) \n",
    "Standard_Deviation_training = np.std(training_scores, axis=1) \n",
    "  \n",
    "# Mean and Standard Deviation of testing scores \n",
    "mean_testing = np.mean(testing_scores, axis=1) \n",
    "Standard_Deviation_testing = np.std(testing_scores, axis=1) \n",
    "  \n",
    "# dotted blue line is for training scores and green line is for cross-validation score \n",
    "plt.plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\") \n",
    "plt.plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\") \n",
    "  \n",
    "# Drawing plot \n",
    "plt.title(\"LEARNING CURVE FOR SVM Classifier\") \n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\") \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    C: Inverse of the strength of regularization.\n",
    "\n",
    "Behavior: As the value of ‘c’ increases the model gets overfits.\n",
    "\n",
    "As the value of ‘c’ decreases the model underfits.\n",
    "\n",
    "2. γ : Gamma (used only for RBF kernel)\n",
    "\n",
    "Behavior: As the value of ‘ γ’ increases the model gets overfits.\n",
    "\n",
    "As the value of ‘ γ’ decreases the model underfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf','linear','poly']}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.0001,0.1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamma parameter is not large but still the model is still overfitting. Hence we are going to tune C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_svm = np.arange(0.0001,0.1,0.01)\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "    SVC(gamma=0.1,kernel='rbf'), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_svm,  scoring=\"accuracy\",param_name=\"C\",cv=5\n",
    ")\n",
    "\n",
    "\n",
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range=parameter_svm\n",
    "plt.title(\"Validation Curve with SVM varying C\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameter_svm = np.arange(0.0001,0.1,0.01)\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "    SVC(gamma=0.1,kernel='rbf'), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_svm,  scoring=\"precision\",param_name=\"C\",cv=5\n",
    ")\n",
    "\n",
    "\n",
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range=parameter_svm\n",
    "plt.title(\"Validation Curve with SVM varying C\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is underfitting when the C value is reduced and overfitting when the values are not too high. From learning curve and validation curve results, it is clear that adding more data can only help increase the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " np.arange(0.001,0.1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_svm = np.arange(0.001,0.1,0.01)\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "    SVC(gamma=0.1,kernel='rbf'), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_svm,  scoring=\"accuracy\",param_name=\"C\",cv=5\n",
    ")\n",
    "\n",
    "\n",
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range=parameter_svm\n",
    "plt.title(\"Validation Curve with SVM varying C\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "gridKNN= {\n",
    "    'n_neighbors':list(range(1,10,2)),\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "\n",
    "knn = GridSearchCV(\n",
    "KNeighborsClassifier(),\n",
    "gridKNN,\n",
    "verbose=1,\n",
    "cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(xtrain_tfidf_ngram, y_train['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "joblib.dump(knn, 'KNN_gridsearch_NLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnModel=knn\n",
    "knnModel.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_res=y_train_res_01.map({ 1:'Existing Customer',  0:'Attrited Customer'})\n",
    "\n",
    "parameter_knn= [3,4,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "    KNeighborsClassifier(metric= 'manhattan',weights= 'distance'), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_knn,  scoring=\"precision\",param_name=\"n_neighbors\",cv=5\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range= [3,4,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "plt.title(\"Validation Curve with KNN\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "# lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions1 = knnModel.best_estimator_.predict(xvalid_tfidf_ngram)\n",
    "# knn_predictions1 = np.round(knn_predictions1)\n",
    "print('knnoost Test Set')\n",
    "print('Accuracy: %.2f' % ((knn_predictions1 == y_cv['label']).mean()*100))\n",
    "\n",
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, knnModel.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, knnModel.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, knnModel.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, knnModel.best_estimator_.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "\n",
    "\n",
    "# # print('Confusion matrix: \\n', cm)\n",
    "# print('TP: ', conf_mat_svm[1,1])\n",
    "# print('TN: ', conf_mat_svm[0,0])\n",
    "# print('FP: ', conf_mat_svm[0,1])\n",
    "# print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_cv['label'], model))\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv['label'], knn_predictions1)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv['label'], knn_predictions1,)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv['label'], knn_predictions1)*100,'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, it is evident that the model performs the best at K=3. However, we notice that training and precision is 99 whereas validation precision is 76. This is clear case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# gridKNN= {\n",
    "#     'n_neighbors':list(range(1,10,2)),\n",
    "#     'weights':['uniform','distance'],\n",
    "#     'metric':['euclidean','manhattan']\n",
    "# }\n",
    "\n",
    "knn7 = KNeighborsClassifier(\n",
    "n_neighbors=7,\n",
    "    metric='euclidean', weights= 'distance')\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn7.fit(xtrain_tfidf_ngram, y_train['label']) \n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(knn7, 'KNN_gridsearch_NLP_K7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_predictions7 = knn7.predict(xvalid_tfidf_ngram)\n",
    "# knn_predictions1 = np.round(knn_predictions1)\n",
    "print('knnoost Test Set')\n",
    "print('Accuracy: %.2f' % ((knn_predictions1 == y_cv['label']).mean()*100))\n",
    "\n",
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, knn7.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, knn7.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, knn7.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, knn7.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "\n",
    "\n",
    "# # print('Confusion matrix: \\n', cm)\n",
    "# print('TP: ', conf_mat_svm[1,1])\n",
    "# print('TN: ', conf_mat_svm[0,0])\n",
    "# print('FP: ', conf_mat_svm[0,1])\n",
    "# print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_cv['label'], model))\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv['label'], knn_predictions7)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv['label'], knn_predictions7,)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv['label'], knn_predictions7)*100,'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [5, 6, 7, 8],\n",
    "        }\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n",
    "                            nthread=1, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 25)\n",
    "\n",
    "search = RandomizedSearchCV(xgb_clf, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(x_train, y_train), random_state=25)\n",
    "\n",
    "search.fit(xtrain_tfidf_ngram, y_train)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(search.best_params_)\n",
    "\n",
    "joblib.dump(xgb_clf, 'xgb_gridsearchNLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 2,\n",
    "        'subsample': 1, #prevents overfitting\n",
    "        'colsample_bytree': 0.8,\n",
    "        'max_depth': 8,\n",
    "        }\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n",
    "                            \n",
    "                            nthread=1, use_label_encoder=False, eval_metric=[\"error\", \"logloss\"])\n",
    "\n",
    "xgb_clf.fit(xtrain_tfidf_ngram, y_train,eval_set = [(xtrain_tfidf_ngram, y_train), (xvalid_tfidf_ngram, y_cv)])\n",
    "# print('Best hyperparameters:')\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # print('Confusion matrix: \\n', cm)\n",
    "# print('TP: ', conf_mat_svm[1,1])\n",
    "# print('TN: ', conf_mat_svm[0,0])\n",
    "# print('FP: ', conf_mat_svm[0,1])\n",
    "# print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_test, model))\n",
    "xgb_predictions = xgb_clf.predict(xvalid_tfidf_ngram)\n",
    "xgb_predictions = np.round(xgb_predictions)\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv, xgb_predictions)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv, xgb_predictions)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv, xgb_predictions)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('XGBoost Test Set')\n",
    "print('Accuracy: %.2f' % ((xgb_predictions == y_cv['label']).mean()*100))\n",
    "\n",
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, xgb_clf.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, xgb_clf.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, xgb_clf.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, xgb_clf.predict(xtrain_tfidf_ngram))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_cv, xgb_predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "fprTrain, tprTrain, thresholdTrain = metrics.roc_curve(y_train, xgb_clf.predict(xtrain_tfidf_ngram))\n",
    "roc_aucTrain = metrics.auc(fprTrain, tprTrain)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Test AUC = %0.2f' % roc_auc,color=\"orange\")\n",
    "# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.plot(fprTrain, tprTrain, 'b', label = 'AUC Train = %0.2f' % roc_aucTrain)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# # method II: ggplot\n",
    "# from ggplot import *\n",
    "# df = pd.DataFrame(dict(fpr = fpr, tpr = tpr))\n",
    "# ggplot(df, aes(x = 'fpr', y = 'tpr')) + geom_line() + geom_abline(linetype = 'dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [5, 6, 7, 8],\n",
    "    'n_estimators' : range(50, 400, 50)\n",
    "        }\n",
    "\n",
    "xgb_clf3 = xgb.XGBClassifier(learning_rate=0.3,  objective='binary:logistic',\n",
    "                            nthread=1, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 25)\n",
    "\n",
    "search2 = RandomizedSearchCV(xgb_clf3, param_distributions=params, n_iter=param_comb, scoring='precision', n_jobs=4, cv=skf.split(x_train, y_train), random_state=25)\n",
    "\n",
    "search2.fit(xtrain_tfidf_ngram, y_train)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "print(search2.best_params_)\n",
    "\n",
    "joblib.dump(xgb_clf3, 'xgb_gridsearchNLP_Precision_estimators.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf3.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params={'subsample'= 1.0, 'n_estimators'= 50, 'min_child_weight'= 1, 'max_depth'= 5, 'gamma'= 0.5, 'colsample_bytree'= 1.0}\n",
    "\n",
    "xgb_clf4 = xgb.XGBClassifier(learning_rate=0.3, \n",
    "                             subsample= 1.0, n_estimators= 50, min_child_weight= 1, max_depth= 5, gamma= 0.5, colsample_bytree= 1.0,\n",
    "                            \n",
    "                            nthread=1, use_label_encoder=False, eval_metric=[\"error\", \"logloss\"])\n",
    "\n",
    "xgb_clf4.fit(xtrain_tfidf_ngram, y_train,eval_set = [(xtrain_tfidf_ngram, y_train), (xvalid_tfidf_ngram, y_cv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # print('Confusion matrix: \\n', cm)\n",
    "# print('TP: ', conf_mat_svm[1,1])\n",
    "# print('TN: ', conf_mat_svm[0,0])\n",
    "# print('FP: ', conf_mat_svm[0,1])\n",
    "# print('FN: ', conf_mat_svm[1,0])\n",
    "\n",
    "# print('Classification report: \\n', metrics.classification_report(y_test, model))\n",
    "xgb_predictions4 = xgb_clf4.predict(xvalid_tfidf_ngram)\n",
    "xgb_predictions4 = np.round(xgb_predictions4)\n",
    "print('Accuracy_Score:',metrics.accuracy_score(y_cv, xgb_predictions4)*100,'%')\n",
    "\n",
    "\n",
    "print('precision_score:',metrics.precision_score(y_cv, xgb_predictions4)*100,'%')\n",
    "\n",
    "print('recall_score:',metrics.recall_score(y_cv, xgb_predictions4)*100,'%')\n",
    "\n",
    "xgb_predictions4 = xgb_clf4.predict(xvalid_tfidf_ngram)\n",
    "xgb_predictions4 = np.round(xgb_predictions4)\n",
    "print('XGBoost Test Set')\n",
    "print('Accuracy: %.2f' % ((xgb_predictions4 == y_cv['label']).mean()*100))\n",
    "\n",
    "print('Training Accuracy_Score:',metrics.accuracy_score(y_train, xgb_clf4.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training Recall:',metrics.recall_score(y_train, xgb_clf4.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training precision_score:',metrics.precision_score(y_train, xgb_clf4.predict(xtrain_tfidf_ngram))*100,'%')\n",
    "print('Training F1 Score:',metrics.f1_score(y_train, xgb_clf4.predict(xtrain_tfidf_ngram))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_cv, xgb_predictions4)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "fprTrain, tprTrain, thresholdTrain = metrics.roc_curve(y_train, xgb_clf4.predict(xtrain_tfidf_ngram))\n",
    "roc_aucTrain = metrics.auc(fprTrain, tprTrain)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Test AUC = %0.2f' % roc_auc,color=\"orange\")\n",
    "# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.plot(fprTrain, tprTrain, 'b', label = 'AUC Train = %0.2f' % roc_aucTrain)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# # method II: ggplot\n",
    "# from ggplot import *\n",
    "# df = pd.DataFrame(dict(fpr = fpr, tpr = tpr))\n",
    "# ggplot(df, aes(x = 'fpr', y = 'tpr')) + geom_line() + geom_abline(linetype = 'dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning parameter learning_rate and max_depth as the model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.1, 0.4, 0.1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_train_res=y_train_res_01.map({ 1:'Existing Customer',  0:'Attrited Customer'})\n",
    "\n",
    "parameter_xgb=  range(50, 400, 50)\n",
    "from sklearn.model_selection import validation_curve\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/validation_curve.html\n",
    "training_scores,testing_scores = validation_curve(\n",
    "   xgb.XGBClassifier(learning_rate=0.1, \n",
    "                             subsample= 1.0,  min_child_weight= 1, max_depth= 5, gamma= 0.5, colsample_bytree= 1.0,\n",
    "                            \n",
    "                            nthread=1, use_label_encoder=False, eval_metric=[\"error\", \"logloss\"]), xtrain_tfidf_ngram, y_train['label'],\n",
    "    param_range=parameter_xgb,  scoring=\"precision\",param_name=\"n_estimators\",cv=5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_scores_mean = np.mean(training_scores, axis=1)\n",
    "train_scores_std = np.std(training_scores, axis=1)\n",
    "test_scores_mean = np.mean(testing_scores, axis=1)\n",
    "test_scores_std = np.std(testing_scores, axis=1)\n",
    "param_range= range(50, 400, 50)\n",
    "plt.title(\"Validation Curve with XGB\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "# lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
